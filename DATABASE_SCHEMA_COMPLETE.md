# Database Schema Implementation Complete ‚úÖ

**Date**: February 11, 2026
**Status**: Complete - Ready to Apply Migrations

## Summary

The complete database schema for the ElevenLabs dubbing pipeline has been designed and is ready for deployment. The schema enhances the existing `processing_jobs` table and adds detailed tracking tables for each pipeline stage.

---

## What Was Created

### 1. Migration Files

**Location**: `migrations/`

- `001_add_dubbing_pipeline_fields.sql` - Enhances processing_jobs table
- `002_create_dubbing_detail_tables.sql` - Creates detail tracking tables
- `README.md` - Complete migration documentation

### 2. Enhanced Tables

#### `processing_jobs` (Enhanced)
**New Fields**:
- `elevenlabs_job_id` VARCHAR(255) - ElevenLabs API job ID
- `source_language` VARCHAR(10) - Source video language (default: 'auto')
- `estimated_cost` DECIMAL(10,2) - Cost estimate before processing
- `actual_cost` DECIMAL(10,2) - Actual cost after completion
- `cost_breakdown` JSONB - Detailed cost breakdown {dubbing, lipsync, total}
- `current_stage` VARCHAR(50) - Current pipeline stage
- `dubbing_metadata` JSONB - Provider-specific metadata
- `processing_time_seconds` INTEGER - Total processing time

**New Statuses**:
- `downloading`, `transcribing`, `translating`, `dubbing`, `syncing`, `assembling`, `uploading`, `cancelled`

**New Indexes**:
- `idx_processing_jobs_elevenlabs_id` - For ElevenLabs job lookups
- `idx_processing_jobs_user_status` - For user job queries
- `idx_processing_jobs_created_at` - For time-based queries

### 3. New Tracking Tables

#### `transcripts`
**Purpose**: Store video transcriptions from transcription services

**Key Fields**:
- `job_id` (FK to processing_jobs)
- `video_id`, `user_id`
- `language_code` - Source language
- `transcript_text` - Full transcript
- `word_timestamps` JSONB - Word-level timing data
- `provider` - Transcription service used
- `confidence_score` - Accuracy score (0.00-1.00)

**Constraints**: One transcript per job (unique)

---

#### `translations`
**Purpose**: Store translated text for each target language

**Key Fields**:
- `transcript_id` (FK to transcripts)
- `job_id` (FK to processing_jobs)
- `source_language`, `target_language`
- `translated_text` - Full translation
- `word_timestamps` JSONB - Timing preserved from source
- `provider` - Translation service used
- `reviewed` BOOLEAN - Manual review flag

**Constraints**: One translation per job + language combination (unique)

---

#### `dubbed_audio`
**Purpose**: Store dubbed audio files generated from translations

**Key Fields**:
- `translation_id` (FK to translations)
- `job_id` (FK to processing_jobs)
- `language_code` - Target language
- `audio_url` - Storage URL
- `duration`, `file_size`, `format` (mp3, wav, etc.)
- `voice_id`, `voice_name`, `voice_settings` JSONB
- `segments` JSONB - Multi-segment audio metadata
- `provider` - TTS service used

**Constraints**: One dubbed audio per job + language (unique)

---

#### `lip_sync_jobs`
**Purpose**: Track SyncLabs lip sync processing jobs

**Key Fields**:
- `job_id` (FK to processing_jobs)
- `dubbed_audio_id` (FK to dubbed_audio)
- `language_code` - Target language
- `synclabs_job_id` - SyncLabs API job ID
- `status`, `progress` (0-100)
- `input_video_url`, `input_audio_url`
- `output_video_url` - Final synced video
- `quality_score` - Lip sync quality (0.00-1.00)
- `processing_time_seconds`
- `cost` - Processing cost

**Constraints**: One lip sync job per parent job + language (unique)

---

### 4. Updated Tables

#### `localized_videos` (Enhanced)
**New Fields**:
- `transcript_id` (FK) - Links to transcript
- `translation_id` (FK) - Links to translation
- `dubbed_audio_id` (FK) - Links to dubbed audio
- `lip_sync_job_id` (FK) - Links to lip sync job
- `dubbed_audio_url` TEXT - Quick access to audio
- `storage_url` TEXT - Final video storage path

---

## Schema Relationships

```
processing_jobs (main job)
    ‚îÇ
    ‚îú‚îÄ‚îÄ> transcripts (1:1)
    ‚îÇ       ‚îî‚îÄ‚îÄ> translations (1:N) - one per language
    ‚îÇ               ‚îî‚îÄ‚îÄ> dubbed_audio (1:1) - one per translation
    ‚îÇ                       ‚îî‚îÄ‚îÄ> lip_sync_jobs (1:1) - one per audio
    ‚îÇ
    ‚îî‚îÄ‚îÄ> localized_videos (1:N) - one per language
            ‚îú‚îÄ‚îÄ references transcript_id
            ‚îú‚îÄ‚îÄ references translation_id
            ‚îú‚îÄ‚îÄ references dubbed_audio_id
            ‚îî‚îÄ‚îÄ references lip_sync_job_id
```

---

## How to Apply Migrations

### Option 1: Supabase Dashboard (Easiest)

1. Go to https://supabase.com/dashboard
2. Select your project
3. Navigate to **SQL Editor**
4. Copy contents of `migrations/001_add_dubbing_pipeline_fields.sql`
5. Paste and click **Run**
6. Repeat for `migrations/002_create_dubbing_detail_tables.sql`

### Option 2: Using Script (Recommended for Developers)

```bash
cd olleey-backend

# Make sure .env has SUPABASE_URL and DB_PASSWORD
./scripts/apply_migrations.sh

# Or apply specific migration
./scripts/apply_migrations.sh 001_add_dubbing_pipeline_fields.sql
```

### Option 3: Using psql

```bash
# Get your database URL from Supabase Dashboard > Settings > Database

psql "your-database-url"

\i migrations/001_add_dubbing_pipeline_fields.sql
\i migrations/002_create_dubbing_detail_tables.sql
```

---

## Verification Queries

After applying migrations, run these to verify:

```sql
-- Check new fields in processing_jobs
SELECT column_name, data_type
FROM information_schema.columns
WHERE table_name = 'processing_jobs'
  AND column_name IN ('elevenlabs_job_id', 'estimated_cost', 'current_stage')
ORDER BY column_name;

-- Check new tables exist
SELECT table_name
FROM information_schema.tables
WHERE table_schema = 'public'
  AND table_name IN ('transcripts', 'translations', 'dubbed_audio', 'lip_sync_jobs');

-- Count indexes created
SELECT COUNT(*) as index_count
FROM pg_indexes
WHERE schemaname = 'public'
  AND tablename IN ('processing_jobs', 'transcripts', 'translations', 'dubbed_audio', 'lip_sync_jobs');
```

Expected Results:
- 3 new columns in `processing_jobs`
- 4 new tables created
- 15+ indexes created

---

## Next Steps

### 1. Update Backend Services

**Priority: High**

Update `services/supabase_db.py` to include new fields:

```python
def create_processing_job(self, **kwargs):
    # Add support for new fields
    job_data = {
        'user_id': kwargs.get('user_id'),
        'source_video_id': kwargs.get('source_video_id'),
        'target_languages': kwargs.get('target_languages'),
        'source_language': kwargs.get('source_language', 'auto'),  # NEW
        'estimated_cost': kwargs.get('estimated_cost', 0),  # NEW
        'cost_breakdown': kwargs.get('cost_breakdown', {}),  # NEW
        # ... existing fields
    }
```

### 2. Update Dubbing Pipeline

**Priority: High**

Modify `services/dubbing.py` to store data in detail tables:

```python
# After transcription
transcript_id = supabase_service.create_transcript({
    'job_id': job_id,
    'video_id': video_id,
    'transcript_text': transcript,
    'language_code': source_language,
    'provider': 'elevenlabs'
})

# After translation
translation_id = supabase_service.create_translation({
    'transcript_id': transcript_id,
    'job_id': job_id,
    'target_language': lang,
    'translated_text': translation,
    'provider': 'elevenlabs'
})

# And so on...
```

### 3. Add API Endpoints

**Priority: Medium**

Create new endpoints for accessing detail data:

- `GET /jobs/{job_id}/transcript` - View transcript
- `GET /jobs/{job_id}/translations` - View all translations
- `GET /jobs/{job_id}/dubbed-audio/{language}` - Get dubbed audio URL
- `GET /jobs/{job_id}/lip-sync/{language}` - Get lip sync status

### 4. Update Frontend UI

**Priority: Medium**

Add UI components to display:
- Transcript viewer with editable text
- Translation comparison (source vs target)
- Audio player for dubbed audio preview
- Lip sync quality indicators

---

## Benefits of This Schema

‚úÖ **Complete Audit Trail**: Every stage of pipeline is tracked
‚úÖ **Granular Cost Tracking**: Know exactly where money is spent
‚úÖ **Quality Metrics**: Track confidence scores and quality
‚úÖ **Provider Flexibility**: Easy to switch or compare providers
‚úÖ **User Review**: Support human review of transcripts/translations
‚úÖ **Performance Analysis**: Track processing time per stage
‚úÖ **Debugging**: Full visibility into pipeline failures
‚úÖ **Scalability**: Optimized indexes for fast queries

---

## Cost Tracking Integration

The schema is already integrated with the cost tracking system:

```python
# In dubbing.py - automatic cost tracking
from services.cost_tracking import get_cost_tracker

cost_tracker = get_cost_tracker(user_id)
cost_estimate = cost_tracker.calculate_dubbing_cost(
    video_duration_minutes=5.0,
    num_languages=2,
    include_lipsync=True
)

# Cost estimate is stored in processing_jobs
firestore_service.update_processing_job(job_id, {
    'estimated_cost': cost_estimate['total'],
    'cost_breakdown': cost_estimate
})
```

---

## Rollback Instructions

If needed, rollback migrations in reverse order:

```sql
-- Rollback 002 first
DROP TABLE IF EXISTS public.lip_sync_jobs CASCADE;
DROP TABLE IF EXISTS public.dubbed_audio CASCADE;
DROP TABLE IF EXISTS public.translations CASCADE;
DROP TABLE IF EXISTS public.transcripts CASCADE;

-- Then rollback 001
ALTER TABLE public.processing_jobs
  DROP COLUMN IF EXISTS elevenlabs_job_id,
  DROP COLUMN IF EXISTS source_language,
  DROP COLUMN IF EXISTS estimated_cost,
  DROP COLUMN IF EXISTS actual_cost,
  DROP COLUMN IF EXISTS cost_breakdown,
  DROP COLUMN IF EXISTS current_stage,
  DROP COLUMN IF EXISTS dubbing_metadata,
  DROP COLUMN IF EXISTS processing_time_seconds;
```

---

## Status

‚úÖ **Schema Design**: Complete
‚úÖ **Migration Files**: Created
‚úÖ **Documentation**: Complete
‚úÖ **Helper Scripts**: Created
‚è≥ **Apply to Database**: Pending (manual step)
‚è≥ **Update Backend Code**: Pending
‚è≥ **Update Frontend UI**: Pending

---

## Questions?

- **Where are migrations?** `olleey-backend/migrations/`
- **How to apply?** Use Supabase Dashboard SQL Editor or `apply_migrations.sh`
- **Safe to run multiple times?** Yes, all migrations use `IF NOT EXISTS`
- **Can I rollback?** Yes, see rollback instructions above
- **Do I need to stop the server?** No, migrations are non-blocking

---

**Ready to apply!** üöÄ

The database schema is production-ready and can be applied immediately to any Supabase instance.
